{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc678649",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba49eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b759f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055b5f4",
   "metadata": {},
   "source": [
    "# NYCOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb3fc9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097bbbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110226, 21)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#read in sales data, already subsetted to include Manhattan only\n",
    "sales17 = pd.read_csv(\"INPUT_nycod/2017_manhattan.csv\", skiprows=5, header=None).dropna(how='all')\n",
    "sales18 = pd.read_csv(\"INPUT_nycod/2018_manhattan.csv\", skiprows=5, header=None).dropna(how='all')\n",
    "sales19 = pd.read_csv(\"INPUT_nycod/2019_manhattan.csv\", skiprows=5, header=None).dropna(how='all')\n",
    "sales20 = pd.read_csv(\"INPUT_nycod/2020_manhattan.csv\", skiprows=7, header=None).dropna(how='all')\n",
    "sales21 = pd.read_csv(\"INPUT_nycod/2021_manhattan.csv\", skiprows=7, header=None).dropna(how='all')\n",
    "sales22 = pd.read_csv(\"INPUT_nycod/rollingsales_manhattan.csv\", skiprows=1, header=None).dropna(how='all')\n",
    "\n",
    "#concatenate all dfs, check shape\n",
    "allsales = pd.concat((sales17, sales18, sales19, sales20, sales21, sales22), axis=0)\n",
    "allsales = allsales.iloc[:,0:21]\n",
    "print(allsales.shape)\n",
    "\n",
    "#add labels\n",
    "labels = pd.DataFrame(pd.read_csv(\"INPUT_nycod/2021_manhattan.csv\", header=6).columns).T\n",
    "allsales = pd.concat((labels, allsales), axis=0)\n",
    "allsales.columns = allsales.iloc[0] \n",
    "allsales = allsales[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf94123",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474d636",
   "metadata": {},
   "source": [
    "### Dtype handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87dba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert objects to numeric where possible\n",
    "allsales['BOROUGH'] = allsales['BOROUGH'].astype(np.int64)\n",
    "allsales['BLOCK'] = allsales['BLOCK'].astype(np.int64)\n",
    "allsales['LOT'] = allsales['LOT'].astype(np.int64)\n",
    "allsales['RESIDENTIAL\\nUNITS'] = pd.to_numeric(allsales['RESIDENTIAL\\nUNITS'].str.replace(',',''), errors='coerce')\n",
    "allsales['TOTAL \\nUNITS'] = pd.to_numeric(allsales['TOTAL \\nUNITS'].str.replace(',',''), errors='coerce')\n",
    "allsales['LAND \\nSQUARE FEET'] = pd.to_numeric(allsales['LAND \\nSQUARE FEET'].str.replace(',',''), errors='coerce')\n",
    "allsales['GROSS \\nSQUARE FEET'] = pd.to_numeric(allsales['GROSS \\nSQUARE FEET'].str.replace(',',''), errors='coerce')\n",
    "allsales['COMMERCIAL\\nUNITS'] = pd.to_numeric(allsales['COMMERCIAL\\nUNITS'], errors='coerce')\n",
    "allsales['YEAR BUILT'] = pd.to_numeric(allsales['YEAR BUILT'], errors='coerce')\n",
    "\n",
    "#clean sale price\n",
    "prices = []\n",
    "for i in allsales['SALE PRICE']:\n",
    "    prices.append(float(i.replace(\",\", \"\").replace(\"'\", \"\").replace(\"$\", \"\")))\n",
    "allsales['saleprice'] = prices\n",
    "allsales = allsales.drop(columns=['SALE PRICE'])\n",
    "\n",
    "#clean sale date, create 'years old at time of sale' variable\n",
    "import datetime\n",
    "dates = []\n",
    "for i in allsales['SALE DATE']:\n",
    "    dates.append(datetime.datetime.strptime(i, \"%m/%d/%Y\"))\n",
    "allsales['saledate'] = dates\n",
    "                 \n",
    "years = []\n",
    "for i in allsales['SALE DATE']:\n",
    "    years.append(int(i[-4:]))\n",
    "allsales['year_clean'] = years\n",
    "allsales['bldg_age_at_sale_calc'] = allsales.year_clean - allsales['YEAR BUILT']\n",
    "allsales = allsales.drop(columns=['YEAR BUILT'])\n",
    "allsales = allsales.drop(columns=['SALE DATE'])\n",
    "allsales = allsales.drop(columns=['year_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28824625",
   "metadata": {},
   "source": [
    "### Drop bad columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "073bf54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop irrelevant and duplicate columns\n",
    "allsales = allsales.drop(columns=['NEIGHBORHOOD',\n",
    "                                  'BOROUGH', \n",
    "                                  'BUILDING CLASS CATEGORY', \n",
    "                                  'TAX CLASS AT PRESENT', \n",
    "                                  'BUILDING CLASS AT PRESENT',\n",
    "                                  'TAX CLASS AT TIME OF SALE',\n",
    "                                  'BUILDING CLASS\\nAT TIME OF SALE',\n",
    "                                  'EASE-MENT'])\n",
    "\n",
    "#rename columns for interpretability\n",
    "col_mapper = {'RESIDENTIAL\\nUNITS':'units_residential',\n",
    "             'COMMERCIAL\\nUNITS':'units_commercial',\n",
    "             'TOTAL \\nUNITS':'units_total',\n",
    "             'LAND \\nSQUARE FEET': 'area_land',\n",
    "             'GROSS \\nSQUARE FEET': 'area_gross'}\n",
    "\n",
    "allsales = allsales.rename(mapper=col_mapper, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c787de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase and rename\n",
    "allsales.columns = map(str.lower, allsales.columns)\n",
    "allsales.columns = [i.replace('\\n', \" \").replace(\"  \", \" \").replace(\" \",\"\") for i in allsales.columns]\n",
    "\n",
    "allsales = allsales.drop(columns=['area_gross',\n",
    "                     'area_land',\n",
    "                     'units_total',\n",
    "                     'apartmentnumber'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbca18",
   "metadata": {},
   "source": [
    "### Drop bad rows (sale price == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c96a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR VIZ ONLY\n",
    "#missing sale price\n",
    "print(\"total observations: {}\".format(len(allsales)))\n",
    "print(\"non-sale transfers (sale price 0): {}\".format(len(allsales[allsales['saleprice'] == 0])))\n",
    "print(\"sales (sale price > 0): {}\".format(len(allsales[allsales['saleprice'] > 0])))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(allsales[allsales.saleprice > 0].saledate, alpha=0.5, color='lightblue', label='sales', bins=6)\n",
    "plt.hist(allsales[allsales.saleprice == 0].saledate, alpha=0.5, color='darkblue', label='non-sale transfers', bins=6)\n",
    "plt.legend()\n",
    "plt.title('Frequency of property sales vs. non-sale transfers')\n",
    "plt.xlabel('Transaction year')\n",
    "plt.ylabel('Number of transactions')\n",
    "plt.savefig('OUTPUT_visualizations/1.Sales vs. non-sale transfers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa87b9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#SUBSET DATA\n",
    "allsales_use = allsales[allsales['saleprice'] > 0]\n",
    "\n",
    "#after removing zeros\n",
    "allsales_use['log_saleprice'] = np.log(allsales_use.saleprice)\n",
    "allsales_use = allsales_use.drop(columns=['saleprice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf0f38",
   "metadata": {},
   "source": [
    "# PLUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e4103",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4a67cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42695, 92)\n"
     ]
    }
   ],
   "source": [
    "pluto = pd.read_csv(\"INPUT_pluto/pluto_22v3_1.csv\", low_memory=False)\n",
    "pluto = pluto.dropna(how='all')\n",
    "\n",
    "#subset to include Manhattan only\n",
    "man = pluto[pluto['borough']=='MN']\n",
    "print(man.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a5bd1",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0478c",
   "metadata": {},
   "source": [
    "### Dtype handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3bf213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42695, 69)\n"
     ]
    }
   ],
   "source": [
    "#make list of PLUTO categorical columns\n",
    "cate_cols = pd.DataFrame(man.dtypes)[pd.DataFrame(man.dtypes)[0] == 'object']\n",
    "\n",
    "#create binary variables: ltdheight, splitzone, histdist, landmark\n",
    "man['bin_ltdheight'] = abs(1-(man.ltdheight.isna()))\n",
    "man['bin_splitzone'] = abs(1-(man.splitzone.isna()))\n",
    "man['bin_histdist'] = abs(1-(man.histdist.isna()))\n",
    "man['bin_landmark'] = abs(1-(man.landmark.isna()))\n",
    "\n",
    "#drop all categoricals (including the ones we just recoded) but leave address to use for later recodes\n",
    "cate_cols = cate_cols.drop('address')\n",
    "man = man.drop(columns=cate_cols.index) \n",
    "\n",
    "print(man.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241433c",
   "metadata": {},
   "source": [
    "### Add columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6d5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create FAR variable\n",
    "man['far_calc'] = man['bldgarea'] / man['lotarea']\n",
    "man = man.drop(columns=['bldgarea', 'lotarea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da885f",
   "metadata": {},
   "source": [
    "### Drop bad columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784a4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop other irrelevant columns\n",
    "man = man.drop(columns=['notes', \n",
    "                        'firm07_flag', \n",
    "                        'pfirm15_flag', \n",
    "                        'bctcb2020', \n",
    "                        'ct2010', \n",
    "                        'cb2010'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b800659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase and rename\n",
    "man.columns = map(str.lower, man.columns)\n",
    "man.columns = [i.replace('\\n', \" \").replace(\"  \", \" \").replace(\" \",\"\") for i in man.columns]\n",
    "\n",
    "man = man.drop(columns=['xcoord',\n",
    "                     'ycoord',\n",
    "                     'borocode',\n",
    "                     'plutomapid',\n",
    "                     'sanitboro',\n",
    "                     'unitsres',\n",
    "                     'zipcode',\n",
    "                     'areasource',\n",
    "                     'bbl',\n",
    "                     'builtfar',\n",
    "                     'commfar',\n",
    "                      'cd',\n",
    "                     'facilfar',\n",
    "                     'proxcode',\n",
    "                     'residfar',\n",
    "                     'taxmap',\n",
    "                     'unitstotal',\n",
    "                     'spdist3',\n",
    "                     'landuse',\n",
    "                     'easements',\n",
    "                     'bsmtcode',\n",
    "                     'assessland',\n",
    "                     'assesstot',\n",
    "                     'exempttot',\n",
    "                     'condono',\n",
    "                     'lottype',\n",
    "                     'lotfront',\n",
    "                     'lotdepth',\n",
    "                     'bldgfront',\n",
    "                     'bldgdepth',\n",
    "                     'appbbl',\n",
    "                     'tract2010'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "man.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899621b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#corr matrix of the district variables\n",
    "import matplotlib.pyplot as plt\n",
    "f = plt.figure(figsize=(14, 10))\n",
    "plt.matshow(man[['policeprct', 'schooldist', 'healthcenterdistrict', 'sanitdistrict', 'council']].corr(), fignum=f.number)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "plt.xticks(range(0, 5, 1), ['policeprct', 'schooldist', 'healthcenterdistrict', 'sanitdistrict', 'council'], fontsize=10)\n",
    "plt.yticks(range(0, 5, 1), ['policeprct', 'schooldist', 'healthcenterdistrict', 'sanitdistrict', 'council'], fontsize=10)\n",
    "plt.title('Correlation between locational variables')\n",
    "plt.savefig('OUTPUT_visualizations/2.District overlap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec6df4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "block                     int64\n",
       "lot                       int64\n",
       "bct2020                 float64\n",
       "schooldist               object\n",
       "council                  object\n",
       "policeprct               object\n",
       "healthcenterdistrict     object\n",
       "healtharea              float64\n",
       "sanitdistrict            object\n",
       "address                  object\n",
       "comarea                 float64\n",
       "resarea                 float64\n",
       "officearea              float64\n",
       "retailarea              float64\n",
       "garagearea              float64\n",
       "strgearea               float64\n",
       "factryarea              float64\n",
       "otherarea               float64\n",
       "numbldgs                float64\n",
       "numfloors               float64\n",
       "yearbuilt               float64\n",
       "yearalter1              float64\n",
       "yearalter2              float64\n",
       "latitude                float64\n",
       "longitude               float64\n",
       "bin_ltdheight             int32\n",
       "bin_splitzone             int32\n",
       "bin_histdist              int32\n",
       "bin_landmark              int32\n",
       "far_calc                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_cols = ['policeprct','schooldist','healthcenterdistrict', 'sanitdistrict', 'council']\n",
    "for i in binary_cols:\n",
    "    man[i] = man[i].astype(object)\n",
    "    \n",
    "man.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f486e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42694, 30)\n"
     ]
    }
   ],
   "source": [
    "man = man[man.block < man.block.max()]\n",
    "print(man.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ed3a1",
   "metadata": {},
   "source": [
    "# Merge NYCOD and PLUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd24a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(man.block, man.lot, label='Included in PLUTO', color='blue', marker='o')\n",
    "plt.scatter(allsales_use.block, allsales_use.lot, label='Actual sales (NYCOD)', color='lightgray', alpha=0.1, marker='.')\n",
    "plt.xlabel('Block code')\n",
    "plt.ylabel('Lot code')\n",
    "plt.title('Block-Lot codes covered by PLUTO')\n",
    "plt.legend()\n",
    "plt.savefig('OUTPUT_visualizations/3.Borough-Lot coverage.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed348310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block\n",
      "lot\n",
      "address\n"
     ]
    }
   ],
   "source": [
    "for i in allsales_use.columns:\n",
    "    if i in man.columns:\n",
    "        print(i)\n",
    "        \n",
    "man = man.rename(mapper={'block':'block_pluto',\n",
    "                        'lot':'lot_pluto',\n",
    "                        'address':'address_pluto'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ba6f406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transactions (from NYCOD): 88439\n",
      "Successfully mapped to PLUTO via BBL: 49.474%\n"
     ]
    }
   ],
   "source": [
    "#join based on BBL\n",
    "\n",
    "df_1 = allsales_use.merge(man, how='left', left_on=['block', 'lot'], right_on=['block_pluto', 'lot_pluto'])\n",
    "print('Total transactions (from NYCOD): {}'.format(len(df_1)))\n",
    "print(\"Successfully mapped to PLUTO via BBL: {:.3f}%\".format(100*(len(df_1) - len(df_1[df_1.latitude.isna() == True]))/(len(df_1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a5e7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmapped transactions (from NYCOD): 49458\n",
      "Successfully mapped to PLUTO via address: 37.420%\n"
     ]
    }
   ],
   "source": [
    "#join based on address\n",
    "\n",
    "#isolate transactions that were not matched before\n",
    "missed = df_1[df_1['latitude'].isna() == True].iloc[:,:10]\n",
    "\n",
    "#clean address text\n",
    "addresses = []\n",
    "for i in missed.address:\n",
    "    addresses.append(i.split(',')[0])\n",
    "missed['address_clean'] = addresses\n",
    "\n",
    "#merge on address\n",
    "df_2 = missed.merge(man, how='left', left_on=['address_clean'], right_on=['address_pluto'])\n",
    "\n",
    "print('Unmapped transactions (from NYCOD): {}'.format(len(df_2)))\n",
    "print(\"Successfully mapped to PLUTO via address: {:.3f}%\".format(100*(len(df_2) - len(df_2[df_2.latitude.isna() == True]))/(len(df_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c2bf16e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['address_pluto'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m len_bbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_1)\n\u001b[0;32m      3\u001b[0m len_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_2)\n\u001b[1;32m----> 5\u001b[0m df_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatitude\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maddress_pluto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df_2 \u001b[38;5;241m=\u001b[39m df_2[df_2\u001b[38;5;241m.\u001b[39mlatitude\u001b[38;5;241m.\u001b[39misna() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m]\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_clean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_pluto\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m total sales\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(len_all))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4806\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   4807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   4808\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4816\u001b[0m ):\n\u001b[0;32m   4817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4952\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4956\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['address_pluto'] not found in axis\""
     ]
    }
   ],
   "source": [
    "len_all = len(allsales_use)\n",
    "len_bbl = len(df_1)\n",
    "len_add = len(df_2)\n",
    "\n",
    "df_1 = df_1[df_1.latitude.isna() == False].drop(columns=['address_pluto'])\n",
    "df_2 = df_2[df_2.latitude.isna() == False].drop(columns=['address_clean', 'address_pluto'])\n",
    "\n",
    "print('{} total sales'.format(len_all))\n",
    "print('{} matched on BBL'.format(len_bbl))\n",
    "print('{} matched on address'.format(len_add))\n",
    "print('{} unmatched ({:.3f}% of total)'.format((len_all - len_bbl - len_add), 100*(len_all - len_bbl - len_add) / len_all))\n",
    "\n",
    "df = pd.concat((df_1, df_2), axis=0)\n",
    "#drop empty\n",
    "df = df.dropna(how='all', axis=1)\n",
    "df = df.dropna(how='all', axis=0)\n",
    "#final df\n",
    "print('\\n')\n",
    "print('shape of final df: {}'.format(df.shape))\n",
    "df.head()\n",
    "\n",
    "df.to_csv('OUTPUT_cleandata/df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6846845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot corr matrix of all vars\n",
    "\n",
    "f = plt.figure(figsize=(14, 8))\n",
    "plt.matshow(df.corr(), fignum=f.number)\n",
    "plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=10, rotation=90)\n",
    "plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=10)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "plt.title('Correlation between independent variables')\n",
    "plt.savefig('OUTPUT_visualizations/4.Corr matrix for all vars.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bf8913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with > 90% missing data: ['block_pluto_x'] \n",
      "\n",
      "Features with any missing data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>pct_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>block_pluto_x</td>\n",
       "      <td>99.950210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>block_pluto_y</td>\n",
       "      <td>70.275132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>units_residential</td>\n",
       "      <td>68.607957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>units_commercial</td>\n",
       "      <td>58.171247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>block_pluto</td>\n",
       "      <td>29.724868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>numfloors</td>\n",
       "      <td>4.616052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bldg_age_at_sale_calc</td>\n",
       "      <td>3.954321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>far_calc</td>\n",
       "      <td>3.596152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>numbldgs</td>\n",
       "      <td>3.498177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>factryarea</td>\n",
       "      <td>1.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>otherarea</td>\n",
       "      <td>1.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>garagearea</td>\n",
       "      <td>1.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>retailarea</td>\n",
       "      <td>1.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>officearea</td>\n",
       "      <td>1.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>resarea</td>\n",
       "      <td>1.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>comarea</td>\n",
       "      <td>1.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>strgearea</td>\n",
       "      <td>1.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bct2020</td>\n",
       "      <td>0.083519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>yearbuilt</td>\n",
       "      <td>0.001606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>yearalter1</td>\n",
       "      <td>0.001606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>yearalter2</td>\n",
       "      <td>0.001606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              column_name  pct_missing\n",
       "38          block_pluto_x    99.950210\n",
       "39          block_pluto_y    70.275132\n",
       "4       units_residential    68.607957\n",
       "5        units_commercial    58.171247\n",
       "9             block_pluto    29.724868\n",
       "27              numfloors     4.616052\n",
       "7   bldg_age_at_sale_calc     3.954321\n",
       "37               far_calc     3.596152\n",
       "26               numbldgs     3.498177\n",
       "24             factryarea     1.034355\n",
       "25              otherarea     1.034355\n",
       "22             garagearea     1.034355\n",
       "21             retailarea     1.034355\n",
       "20             officearea     1.034355\n",
       "19                resarea     1.034355\n",
       "18                comarea     1.034355\n",
       "23              strgearea     1.034355\n",
       "11                bct2020     0.083519\n",
       "28              yearbuilt     0.001606\n",
       "29             yearalter1     0.001606\n",
       "30             yearalter2     0.001606"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#drop cols with more than 90% missing data\n",
    "cols = pd.DataFrame(df.columns).rename(columns={0:'column_name'})\n",
    "missing = []\n",
    "for i in cols['column_name']:\n",
    "    missing.append(100*df[i].isnull().sum()/len(df[i]))\n",
    "cols['pct_missing'] = missing\n",
    "to_drop = []\n",
    "for i in cols.index:\n",
    "    if cols['pct_missing'][i] > 90:\n",
    "        to_drop.append(cols['column_name'][i])\n",
    "df = df.drop(columns=to_drop)\n",
    "\n",
    "print('Features with > 90% missing data: {} \\n'.format(to_drop))\n",
    "print('Features with any missing data:')\n",
    "display(cols[cols['pct_missing']>0].sort_values('pct_missing', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.saledate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot hist of sale prices\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['log_saleprice'])\n",
    "plt.xlabel('Ln of sale price ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Manhattan residential real estate sales, 2017-2022')\n",
    "plt.savefig('OUTPUT_visualizations/5.Histogram of ln sale price.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b54115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'bct2020_pluto' is 2020 census tract if needed for tabulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe67924",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb582256",
   "metadata": {},
   "source": [
    "# Create four different preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7af64",
   "metadata": {},
   "source": [
    "### Sort chronologically to train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "\n",
    "#sort values by date\n",
    "df = df.sort_values('saledate')\n",
    "df = df.drop(columns=['saledate'])\n",
    "\n",
    "# drop sale price\n",
    "\n",
    "#subset into X and y\n",
    "X = df.drop('log_saleprice', axis=1)\n",
    "y = df['log_saleprice']\n",
    "\n",
    "#split into train test split\n",
    "train_size = round(len(df)*0.75)\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317cc0da",
   "metadata": {},
   "source": [
    "## RF preprocessor, full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes\n",
    "\n",
    "X_train = X_train.drop(columns=['address', 'zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16a9c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_features_full = X_train.select_dtypes([np.number]).columns.tolist()\n",
    "categorical_features_full = X_train.select_dtypes([object]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))])\n",
    "\n",
    "preprocessor_full_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features_full),\n",
    "        ('cat', categorical_transformer, categorical_features_full)])\n",
    "\n",
    "preprocess_full_rf = preprocessor_full_rf.fit(X_train) \n",
    "\n",
    "def preprocessor_full_rf(data):\n",
    "    preprocessed_data = preprocess_full_rf.transform(data)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839c03f",
   "metadata": {},
   "source": [
    "## OLS preprocessor, full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eae1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#turn columns into categorical variables: zip code, police precinct, school district, etc.\n",
    "#ENSURE COLS ARE OHE: ['policeprct', 'schooldist', 'healthcenterdistrict', 'sanitdistrict', 'council']\n",
    "\n",
    "numeric_features_full = X_train.select_dtypes([np.number]).columns.tolist()\n",
    "\n",
    "categorical_features_full = X_train.select_dtypes([object]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor_full_ols = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features_full),\n",
    "        ('cat', categorical_transformer, categorical_features_full)])\n",
    "\n",
    "preprocessor_full_ols = preprocessor_full_ols.fit(X_train) \n",
    "\n",
    "def preprocessor_full_ols(data):\n",
    "    preprocessed_data=preprocessor_full_ols.transform(data)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faef709",
   "metadata": {},
   "source": [
    "## RF preprocessor, subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_features_sub = X_train_imp.select_dtypes([np.number]).columns.tolist()\n",
    "categorical_features_sub = X_train_imp.select_dtypes([object]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))])\n",
    "\n",
    "preprocessor_sub_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features_full),\n",
    "        ('cat', categorical_transformer, categorical_features_full)])\n",
    "\n",
    "preprocess_sub_rf = preprocessor_sub_rf.fit(X_train_imp) \n",
    "\n",
    "def preprocessor_sub_rf(data):\n",
    "    preprocessed_data = preprocess_sub_rf.transform(data)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975be46a",
   "metadata": {},
   "source": [
    "## OLS preprocessor, subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad30f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_sub = X_train_imp.select_dtypes([np.number]).columns.tolist()\n",
    "categorical_features_sub = X_train_imp.select_dtypes([object]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocess_sub_ols = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features_sub),\n",
    "        ('cat', categorical_transformer, categorical_features_sub)])\n",
    "\n",
    "preprocess_sub_ols = preprocess_sub_ols.fit(X_train_imp) \n",
    "\n",
    "def preprocessor_sub_ols(data):\n",
    "    preprocessed_data = preprocess_sub_ols.transform(data)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40741d8b",
   "metadata": {},
   "source": [
    "# Run and compare models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4dcc9",
   "metadata": {},
   "source": [
    "## RF, all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676dd4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest for feat. selection\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {'max_depth': [10, 20, 30, 40, 50],\n",
    "              'n_estimators': [10, 20, 30, 40, 50]}\n",
    "\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid = param_grid, cv=10)\n",
    "\n",
    "grid.fit(preprocessor_full_rf(X_train), y_train)\n",
    "\n",
    "print(\"best median cross-val score: {}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {}\".format(grid.score(preprocessor_full_rf(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_forest = RandomForestRegressor(max_depth = grid.best_params_['max_depth'], n_estimators=grid.best_params_['n_estimators']).fit(preprocessor_full_rf(X_train), y_train)\n",
    "\n",
    "forest_1_predictions = best_forest.predict(preprocessor_full_rf(X_test))\n",
    "\n",
    "feature_names = X_train.columns\n",
    "forest_importances = pd.DataFrame(best_forest.feature_importances_, index=feature_names).sort_values([0], ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(forest_importances.index, forest_importances[0])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature importances from best Random Forest model; max depth {} and n_estimators {}'.format(\n",
    "    grid.best_params_['max_depth'], grid.best_params_['n_estimators']))\n",
    "plt.grid()\n",
    "plt.savefig('OUTPUT_models/featureimportances.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f501d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_imp_cols = forest_importances.index[:20]\n",
    "\n",
    "X_train_imp = X_train[most_imp_cols]\n",
    "X_test_imp = X_test[most_imp_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857c0a3",
   "metadata": {},
   "source": [
    "## RF, subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f249baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=20, max_depth=20).fit(preprocessor_sub_rf(X_train_imp), y_train)\n",
    "#param_grid = {'max_depth': [10, 15, 20, 25],\n",
    "              #'n_estimators': [10, 30, 50, 100]}\n",
    "#grid = GridSearchCV(RandomForestRegressor(), param_grid = param_grid, cv=10)\n",
    "\n",
    "print(\"Train set score: {:.5f}\".format(rf.score(preprocessor_sub_rf(X_train_imp), y_train)))\n",
    "print(\"Mean cross-val score for train set: {:.5f}\".format(cross_val_score(rf, preprocessor_sub_rf(X_train_imp), y_train, scoring='r2').mean()))\n",
    "print(\"Test set score: {:.5f}.\".format(rf.score(preprocessor_sub_rf(X_test_imp), y_test)))\n",
    "\n",
    "forest_2_predictions = rf.predict(preprocessor_sub_rf(X_test_imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c89695",
   "metadata": {},
   "source": [
    "## OLS, all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression().fit(preprocessor_full_ols(X_train), y_train)\n",
    "\n",
    "print(\"Train set score: {:.5f}\".format(lr.score(preprocessor_full_ols(X_train), y_train)))\n",
    "print(\"Mean cross-val score for train set: {:.5f}\".format(cross_val_score(lr, preprocessor_full_ols(X_train), y_train, scoring='r2').mean()))\n",
    "print(\"Test set score: {:.5f}.\".format(lr.score(preprocessor_full_ols(X_test), y_test)))\n",
    "\n",
    "ols_1_predictions = lr.predict(preprocessor_full_ols(X_test))\n",
    "\n",
    "#sort results by coefficient size\n",
    "ols_1_results = pd.concat((pd.DataFrame(X_train.columns), pd.DataFrame(lr.coef_)), axis=1)\n",
    "ols_1_results.columns = ['column', 'coef']\n",
    "ols_1_results = ols_1_results.sort_values('coef', ascending=True)\n",
    "\n",
    "#plot figure\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(ols_1_results.column, ols_1_results.coef)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel('Coefficient size')\n",
    "plt.savefig(\"OUTPUT_models/fullolscoefs.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ba71d",
   "metadata": {},
   "source": [
    "## OLS, subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7999f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression().fit(preprocessor_sub_ols(X_train_imp), y_train)\n",
    "\n",
    "print(\"Train set score: {:.5f}\".format(lr.score(preprocessor_sub_ols(X_train_imp), y_train)))\n",
    "print(\"Mean cross-val score for train set: {:.5f}\".format(cross_val_score(lr, preprocessor_sub_ols(X_train_imp), y_train, scoring='r2').mean()))\n",
    "print(\"Test set score: {:.5f}.\".format(lr.score(preprocessor_sub_ols(X_test_imp), y_test)))\n",
    "\n",
    "ols_2_predictions = lr.predict(preprocessor_sub_ols(X_test_imp))\n",
    "\n",
    "#sort results by coefficient size\n",
    "ols_2_results = pd.concat((pd.DataFrame(X_train_imp.columns), pd.DataFrame(lr.coef_)), axis=1)\n",
    "ols_2_results.columns = ['column', 'coef']\n",
    "ols_2_results = ols_2_results.sort_values('coef', ascending=True)\n",
    "\n",
    "#plot figure\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(ols_2_results.column, ols_2_results.coef)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel('Coefficient size')\n",
    "plt.savefig(\"OUTPUT_models/subsetcoefs.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b96c0",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27313be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['Model'] = ['RF, all features', 'RF, 20 features', 'OLS, all features', 'OLS, 20 features']\n",
    "results = results.set_index('Model')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "#RMSE AND %RMSE\n",
    "results['RMSE'] = [math.sqrt(mean_squared_error(forest_1_predictions, y_test)), \n",
    "                  math.sqrt(mean_squared_error(forest_2_predictions, y_test)), \n",
    "                  math.sqrt(mean_squared_error(ols_1_predictions, y_test)), \n",
    "                  math.sqrt(mean_squared_error(ols_2_predictions, y_test))]\n",
    "\n",
    "def percent_rmse(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0))\n",
    "    return rmspe\n",
    "results['%RMSE'] = [percent_rmse(y_test, forest_1_predictions),\n",
    "                   percent_rmse(y_test, forest_2_predictions),\n",
    "                   percent_rmse(y_test, ols_1_predictions),\n",
    "                   percent_rmse(y_test, ols_2_predictions)]\n",
    "\n",
    "#MAE AND MAPE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "results['MAE'] = [mean_absolute_error(y_test, forest_1_predictions).round(2), \n",
    "                  mean_absolute_error(y_test, forest_2_predictions), \n",
    "                  mean_absolute_error(y_test, ols_1_predictions), \n",
    "                  mean_absolute_error(y_test, ols_2_predictions)]\n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true))*100\n",
    "    return mape\n",
    "\n",
    "results['%MAE'] = [MAPE(y_test, forest_1_predictions),\n",
    "                   MAPE(y_test, forest_2_predictions),\n",
    "                   MAPE(y_test, ols_1_predictions),\n",
    "                   MAPE(y_test, ols_2_predictions)]\n",
    "\n",
    "#R2 (COEFFICIENT OF DETERMINATION)\n",
    "from sklearn.metrics import r2_score\n",
    "results['R-squared'] = [r2_score(y_test, forest_1_predictions), \n",
    "                  r2_score(y_test, forest_2_predictions), \n",
    "                  r2_score(y_test, ols_1_predictions), \n",
    "                  r2_score(y_test, ols_2_predictions)]\n",
    "\n",
    "results = results.round(3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in results.columns:\n",
    "    plt.scatter(results.index, results[i], label=i)\n",
    "plt.legend()\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Final model performance')\n",
    "plt.savefig('OUTPUT_models/model_performance.png')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65304b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hit rates\n",
    "y_test_for_hit_rate = list(y_test)\n",
    "hit_rates = pd.DataFrame(index=['RF, all features', 'RF, 20 features', 'OLS, all features', 'OLS, 20 features'], \n",
    "                         columns=['Within 1%', 'Within 5%', 'Within 10%'])\n",
    "predictions = [forest_1_predictions, forest_2_predictions, ols_1_predictions, ols_2_predictions]\n",
    "def predict_hit_rate(y_true, y_pred, margin):\n",
    "    a = 0\n",
    "    rate_tmp = []\n",
    "    for i in y_pred:\n",
    "        if i >= y_true.iloc[a]*(1-margin):\n",
    "            if i >= y_true.iloc[a]*(1+margin):\n",
    "                rate_tmp.append(1)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            rate_tmp.append(0)\n",
    "        a += 1\n",
    "    return(sum(rate_tmp) / len(rate_tmp))\n",
    "\n",
    "hit_rates['Within 1%']['RF, all features'] = predict_hit_rate(y_test, forest_1_predictions, 0.01)*100\n",
    "hit_rates['Within 1%']['RF, 20 features'] = predict_hit_rate(y_test, forest_2_predictions, 0.01)*100\n",
    "hit_rates['Within 1%']['OLS, all features'] = predict_hit_rate(y_test, ols_1_predictions, 0.01)*100\n",
    "hit_rates['Within 1%']['OLS, 20 features'] = predict_hit_rate(y_test, ols_2_predictions, 0.01)*100\n",
    "hit_rates['Within 5%']['RF, all features'] = predict_hit_rate(y_test, forest_1_predictions, 0.05)*100\n",
    "hit_rates['Within 5%']['RF, 20 features'] = predict_hit_rate(y_test, forest_2_predictions, 0.05)*100\n",
    "hit_rates['Within 5%']['OLS, all features'] = predict_hit_rate(y_test, ols_1_predictions, 0.05)*100\n",
    "hit_rates['Within 5%']['OLS, 20 features'] = predict_hit_rate(y_test, ols_2_predictions, 0.05)*100\n",
    "hit_rates['Within 10%']['RF, all features'] = predict_hit_rate(y_test, forest_1_predictions, 0.10)*100\n",
    "hit_rates['Within 10%']['RF, 20 features'] = predict_hit_rate(y_test, forest_2_predictions, 0.10)*100\n",
    "hit_rates['Within 10%']['OLS, all features'] = predict_hit_rate(y_test, ols_1_predictions, 0.10)*100\n",
    "hit_rates['Within 10%']['OLS, 20 features'] = predict_hit_rate(y_test, ols_2_predictions, 0.10)*100\n",
    "\n",
    "hit_rates.astype(float).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in hit_rates.columns:\n",
    "    plt.scatter(hit_rates.index, hit_rates[i], label=i)\n",
    "plt.legend()\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Percent of predictions with margin of actuals')\n",
    "plt.title('Model hit rates')\n",
    "plt.savefig('OUTPUT_models/hit_rates.png')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
